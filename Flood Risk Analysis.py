# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Abvk9EZO3tW9j88MiD3wLmDUpRkKkBmY
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly as px
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings("ignore")

data = pd.read_csv('/flood.csv')
data.head()

data.info()

data.describe()

# check for outliers

numerical_features = data.select_dtypes(include=['float64', 'int64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_features, 1):
    plt.subplot(2, (len(numerical_features) + 1) // 2, i)
    sns.boxplot(x=data[col])
    plt.title(f'Box Plot for {col}')
    plt.xlabel(col)

plt.tight_layout()
plt.show()

# Data distrbuition

columns = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',
       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',
       'Siltation', 'AgriculturalPractices', 'Encroachments',
       'IneffectiveDisasterPreparedness', 'DrainageSystems',
       'CoastalVulnerability', 'Landslides', 'Watersheds',
       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',
       'InadequatePlanning', 'PoliticalFactors', 'FloodProbability']

data[columns].hist(bins=30, figsize=(15, 10))

plt.tight_layout()

plt.show()

def cap_outliers(data, col):
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    data[col] = data[col].clip(lower_bound, upper_bound)
for col in numerical_features:
    cap_outliers(data, col)

plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_features, 1):
    plt.subplot(2, (len(numerical_features) + 1) // 2, i)
    sns.boxplot(x=data[col])
    plt.title(f'Box Plot for {col}')
    plt.xlabel(col)

plt.tight_layout()
plt.show()

corr_matrix = data[['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',
                     'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',
                     'Siltation', 'AgriculturalPractices', 'Encroachments',
                     'IneffectiveDisasterPreparedness', 'DrainageSystems',
                     'CoastalVulnerability', 'Landslides', 'Watersheds',
                     'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',
                     'InadequatePlanning', 'PoliticalFactors', 'FloodProbability']].corr()

# Plot heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5,
            cbar_kws={'shrink': 0.8}, annot_kws={'size': 10})
plt.title('Correlation Heatmap', fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0, ha='right')
plt.tight_layout()
plt.show()

data['FloodProbability'] = data['FloodProbability'].apply(lambda x : 1 if x >=0.5 else 0)

selected_vars = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement', 'Urbanization']
sns.pairplot(data, hue='FloodProbability', vars=selected_vars)

data['FloodProbability'].value_counts()

X = data.drop(columns=['FloodProbability'])
y = data['FloodProbability']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report,confusion_matrix, ConfusionMatrixDisplay

model = Sequential([
    Dense(64, activation='relu',  input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')

])

 # Output layer for binary classification

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

print("Total params: 3,457 (13.50 KB)")

print("Trainable params: 3,457 (13.50 KB)")

print("Non-trainable params: 0 (0.00 B)")

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
history = model.fit(X_train, y_train, epochs=50, batch_size=256, validation_split=0.2, verbose=1)

y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5).astype(int)

cm = confusion_matrix(y_test, y_pred, labels=[1, 0])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 0])
disp.plot(cmap=plt.cm.Blues)
plt.show()

